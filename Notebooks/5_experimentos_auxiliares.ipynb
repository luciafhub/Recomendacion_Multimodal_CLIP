{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Experimentos auxiliares para el TFG\n\nEste notebook recoge una serie de bloques de código utilizados como apoyo durante el desarrollo del Trabajo de Fin de Grado. Incluye tareas complementarias como la traducción automática de reseñas al inglés, la generación de embeddings textuales con BERT, el cálculo de similitudes entre imágenes y textos con CLIP y el procesamiento por lotes de datos para evitar cuelgues del sistema. Aunque no forman parte del pipeline principal del sistema de recomendación, estos experimentos y utilidades permitieron tomar decisiones clave sobre qué representaciones utilizar y cómo preprocesar los datos de forma eficiente.","metadata":{}},{"cell_type":"markdown","source":"# Traducción Automática de Reseñas con MarianMT\n\nEste bloque traduce automáticamente los textos de reseñas de restaurantes del español al inglés utilizando\nel modelo MarianMT de Helsinki-NLP. El objetivo principal es adaptar los datos a modelos de recomendación\nmultimodal que utilizan embeddings generados con CLIP.\n\nCLIP fue entrenado mayoritariamente con datos en inglés, por lo que se sospecha que las arquitecturas\nque usan embeddings CLIP de texto funcionarán mejor si las reseñas están traducidas al inglés. Estas\narquitecturas incluyen:\n\n- CLIPTextRegressor: utiliza solo embeddings CLIP de texto.\n- CLIPMixedRegressor y FullModel: combinan embeddings CLIP de texto e imagen con identificadores de usuario e ítem.\n- MixedModel: mezcla codificación tradicional (IDs) con embeddings de texto CLIP.\n\nTraducir los textos antes de generar los embeddings puede mejorar el rendimiento de estas arquitecturas,\nal alinear mejor el lenguaje de entrada con el vocabulario aprendido por CLIP durante su entrenamiento.","metadata":{}},{"cell_type":"code","source":"\n# ------------------------------\n# Importación de librerías\n# ------------------------------\n\nimport pandas as pd                 \nfrom tqdm.auto import tqdm           \nimport os                            \nimport torch                        \n\n# Modelos y pipeline de traducción\nfrom transformers import MarianMTModel, MarianTokenizer, pipeline \n\n# ------------------------------\n# Comprobación del entorno\n# ------------------------------\n\n# Indica si hay GPU disponible para acelerar el proceso\nprint(\"GPU disponible:\", torch.cuda.is_available())\n\n# ------------------------------\n# 1) Carga del modelo y tokenizer\n# ------------------------------\n\n# Nombre del modelo específico de español a inglés de Helsinki-NLP\nmodel_name = \"Helsinki-NLP/opus-mt-es-en\"\n\n# Se carga el tokenizer con opción rápida (más eficiente)\ntokenizer = MarianTokenizer.from_pretrained(model_name, use_fast=True)\n\n# Se carga el modelo y se transfiere a GPU (cuda)\nmodel = MarianMTModel.from_pretrained(model_name).to(\"cuda\")\n\n# ------------------------------\n# 2) Creación del pipeline de traducción\n# ------------------------------\n\n# Se construye el pipeline indicando explícitamente:\n# - Modelo y tokenizer\n# - Uso de GPU con device=0 (primera GPU disponible)\n# - batch_size grande para acelerar, si hay suficiente VRAM\n# - max_length y num_beams ajustados para rapidez\ntranslator = pipeline(\n    \"translation\",\n    model=model,\n    tokenizer=tokenizer,\n    device=0,               # GPU 0\n    framework=\"pt\",         # PyTorch\n    batch_size=64,\n    max_length=512,\n    num_beams=1,            # búsqueda sin beams para mayor velocidad\n    do_sample=False         # desactiva muestreo aleatorio\n)\n\n# ------------------------------\n# 3) Función de traducción por batches\n# ------------------------------\n\ndef translate_dataset_batched(name, batch_size=64, max_chars=512):\n    \"\"\"\n    Traduce un dataset .pkl con una columna 'text' del español al inglés.\n    Guarda el resultado en otro archivo con sufijo '_en.pkl'.\n\n    Parámetros:\n    - name: nombre base del archivo (sin .pkl)\n    - batch_size: número de ejemplos a traducir por lote\n    - max_chars: longitud máxima de texto por entrada (truncado)\n    \"\"\"\n\n    inp = f\"{name}.pkl\"\n    out = f\"{name}_en.pkl\"\n\n    print(f\"\\n>> Cargando {inp}…\")\n    df = pd.read_pickle(inp)\n\n    # Se obtienen los textos, se rellenan nulos y se convierten a string\n    texts = df[\"text\"].fillna(\"\").astype(str).tolist()\n\n    # Truncado previo para asegurar compatibilidad con el modelo\n    texts = [\n        (t if len(t) <= max_chars else t[:max_chars] + \"...\")\n        for t in texts\n    ]\n\n    # Traducción por lotes\n    translated = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Traduciendo {name}\"):\n        batch = texts[i : i + batch_size]\n        outputs = translator(batch)\n        translated.extend([o[\"translation_text\"] for o in outputs])\n\n    # Se añade la columna traducida al DataFrame\n    df[\"text_en\"] = translated\n\n    print(f\">> Guardando en {out}…\")\n    df.to_pickle(out)\n    print(f\"✅ {name} traducido y guardado.\")\n\n# ------------------------------\n# 4) Traducción de datasets disponibles\n# ------------------------------\n\n# Se aplica la función a los conjuntos de entrenamiento, validación y prueba\nfor split in [\"train_strict\", \"val_strict\", \"test_strict\"]:\n    if os.path.isfile(f\"{split}.pkl\"):\n        translate_dataset_batched(split)\n    else:\n        print(f\"[SKIP] No existe {split}.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generación de Embeddings Textuales con BERT\n\nEste bloque genera representaciones vectoriales (embeddings) a partir de reseñas escritas en texto,\nutilizando el modelo preentrenado BERT (bert-base-uncased). A diferencia de CLIP, que genera\nembeddings multimodales (texto + imagen), BERT está optimizado exclusivamente para lenguaje natural,\npor lo que puede capturar mejor las relaciones semánticas presentes en las reseñas.\n\n\nDurante los experimentos se observó que las imágenes asociadas a los restaurantes no aportaban información relevante para la tarea de recomendación. En cambio, las reseñas textuales demostraron tener un alto poder predictivo. Por ello, se decidió probar un modelo alternativo especializado en lenguaje natural: BERT, con la hipótesis de que generaría embeddings más representativos que CLIP.\n\nLos resultados confirmaron esta sospecha: los embeddings generados con BERT ofrecieron un mejor rendimiento que los obtenidos con CLIP. Esto sugiere que, en este dominio concreto, el contenido lingüístico de las reseñas es el principal factor que determina la satisfacción del usuario.\n\nEl objetivo de este script es, por tanto, reemplazar los embeddings generados con CLIP por embeddings obtenidos con BERT, que se almacenan en una nueva columna llamada \"text_emb\" dentro del DataFrame.","metadata":{}},{"cell_type":"code","source":"# ------------------------------\n# 0) Instalación de librerías\n# ------------------------------\n# Se instalan Transformers y TensorFlow (aunque este script no necesita TF)\n# En entornos como Kaggle o Colab, TensorFlow puede generar conflictos si no se usa\n!pip install -q tensorflow tensorflow-estimator\n!pip install -q transformers\n\n# ------------------------------\n# 1) Importación de librerías\n# ------------------------------\n\nimport pandas as pd                            # Manipulación de DataFrames\nimport numpy as np                             # Operaciones numéricas\nimport torch                                   # Uso de GPU y modelo BERT\nfrom transformers import AutoTokenizer, AutoModel  # Carga del modelo BERT\nfrom tqdm.auto import tqdm                     # Barra de progreso\n\n# ------------------------------\n# 2) Configuración del entorno\n# ------------------------------\n\ndevice     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU si está disponible\nMODEL_NAME = \"bert-base-uncased\"   # Modelo preentrenado de BERT (casing ignorado)\nBATCH_SIZE = 32                    # Procesamiento por lotes\nMAX_LEN    = 128                   # Máximo número de tokens por texto\n\n# ------------------------------\n# 3) Carga de modelo y tokenizador\n# ------------------------------\n\n# Tokenizador que divide el texto en subpalabras y añade tokens especiales\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Modelo BERT que genera los embeddings\nmodel = AutoModel.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()  # Se desactiva el modo entrenamiento (no dropout, no actualiza pesos)\n\n# ------------------------------\n# 4) Mean Pooling\n# ------------------------------\n\ndef mean_pooling(hidden_states, attention_mask):\n    \"\"\"\n    Calcula el embedding medio de las palabras reales (excluyendo padding).\n    \n    hidden_states: salida de BERT (batch_size, seq_len, hidden_dim)\n    attention_mask: máscara con 1s en posiciones válidas y 0s en padding\n    \"\"\"\n    mask = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n    return (hidden_states * mask).sum(1) / mask.sum(1)  # Promedio solo donde mask=1\n\n# ------------------------------\n# 5) Función para obtener embeddings BERT\n# ------------------------------\n\ndef get_bert_embeddings(texts, desc=\"\"):\n    \"\"\"\n    Procesa una lista de textos, tokeniza y genera embeddings con BERT.\n    \n    Parámetros:\n    - texts: lista de cadenas de texto\n    - desc: descripción para mostrar en la barra de progreso\n\n    Devuelve:\n    - matriz NumPy (N, hidden_dim) con los embeddings\n    \"\"\"\n    all_embs = []\n\n    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=f\"📝 Embedding {desc}\"):\n        batch = texts[i : i + BATCH_SIZE]\n\n        # Tokenización por lotes con padding y truncado\n        enc = tokenizer(\n            batch,\n            padding=True,\n            truncation=True,\n            max_length=MAX_LEN,\n            return_tensors=\"pt\"\n        )\n\n        input_ids      = enc[\"input_ids\"].to(device)\n        attention_mask = enc[\"attention_mask\"].to(device)\n\n        # Cálculo de embeddings sin actualizar el modelo\n        with torch.no_grad():\n            out = model(input_ids=input_ids, attention_mask=attention_mask)\n\n        # Aplicar mean pooling\n        emb = mean_pooling(out.last_hidden_state, attention_mask)\n\n        # Convertir a NumPy y guardar\n        all_embs.append(emb.cpu().numpy())\n\n    return np.vstack(all_embs)  # (N, dim)\n\n# ------------------------------\n# 6) Procesamiento de archivos\n# ------------------------------\n\n# Diccionario con rutas a los archivos .pkl de cada split\nFILES = {\n    \"train\": \"/kaggle/input/ny8010/train_v80.pkl\",\n    \"val\":   \"/kaggle/input/ny8010/val_v10.pkl\",\n    \"test\":  \"/kaggle/input/ny8010/test_v10.pkl\"\n}\n\n# Proceso por cada split\nfor split, path in FILES.items():\n    print(f\"\\n🔄 Procesando {split} ({path})…\")\n\n    # Carga del DataFrame\n    df = pd.read_pickle(path)\n\n    # Normalización de textos: asegurar strings y eliminar NaNs\n    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n\n    # Obtener embeddings\n    embs = get_bert_embeddings(df[\"text\"].tolist(), split)\n\n    # Añadir al DataFrame\n    df[\"text_emb_bert\"] = list(embs)\n\n    # Guardar archivo enriquecido\n    out = f\"/kaggle/working/{split}_bert_emb.pkl\"\n    df.to_pickle(out)\n    print(f\"✅ Guardado embeddings en {out}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Arquitectura con la que se verefica que BERT da mejor resultado que CLIP\n\nSe probó la arquitectura 4-MIX-TXT, que combina los identificadores de usuario e ítem con los embeddings textuales de las reseñas. Inicialmente, esta arquitectura se entrenó utilizando embeddings generados con CLIP. Sin embargo, los resultados no fueron del todo satisfactorios: el mejor RMSE en validación fue de 6.68 para el conjunto de Nueva York.\n\nCon el objetivo de mejorar la calidad de los embeddings textuales, se sustituyó CLIP por BERT (bert-base-uncased), un modelo especializado en lenguaje natural. Esta modificación tuvo un impacto positivo claro: el mejor RMSE en validación descendió a 6.03 en Nueva York, una mejora significativa en la precisión del sistema de recomendación.\n\nEste experimento demuestra que, en esta tarea concreta, las imágenes no aportan valor predictivo, y que el texto —representado mediante embeddings BERT— resulta ser el componente clave. Por tanto, reemplazar CLIP por BERT mejora la capacidad del modelo para anticipar la satisfacción del usuario.","metadata":{}},{"cell_type":"code","source":"import time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# ========================\n# CONFIGURACIÓN GENERAL\n# ========================\nstart_total = time.time()\n\nDEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nuse_amp    = DEVICE == \"cuda\"\nEMB_DIM    = 50                 # dimensión de user/item embeddings\nBATCH_SIZE = 128\nLR_LIST    = [1e-5, 1e-4, 5e-4, 1e-3]\nMAX_EPOCHS = 1000\nPATIENCE   = 10\nWD         = 1e-6               # weight_decay\nRESULTS_XLSX = \"Arquitectura_4_MIX_TXTEN_NY_BERT.xlsx\"\n\n# ========================\n# DATASETS\n# ========================\nclass MixedDataset(Dataset):\n    def __init__(self, df):\n        # usuario, restaurante\n        self.u = torch.tensor(df['user_id_new'].values, dtype=torch.long)\n        self.i = torch.tensor(df['restaurant_id_new'].values, dtype=torch.long)\n        # embeddings BERT de texto (dim 768)\n        self.x = torch.tensor(np.vstack(df['text_emb'].values), dtype=torch.float32)\n        # rating\n        self.r = torch.tensor(df['rating'].values, dtype=torch.float32)\n    def __len__(self):\n        return len(self.r)\n    def __getitem__(self, idx):\n        return self.u[idx], self.i[idx], self.x[idx], self.r[idx]\n\n# cargar DataFrames\ntrain = pd.read_pickle(\"/kaggle/input/ny-bert/train_bert_emb.pkl\")\nval   = pd.read_pickle(\"/kaggle/input/ny-bert/val_bert_emb.pkl\")\ntest  = pd.read_pickle(\"/kaggle/input/ny-bert/test_bert_emb.pkl\")\n\n# crear DataLoaders\ntrain_loader = DataLoader(MixedDataset(train), batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(MixedDataset(val),   batch_size=BATCH_SIZE, shuffle=False)\ntest_loader  = DataLoader(MixedDataset(test),  batch_size=BATCH_SIZE, shuffle=False)\n\nn_users = train['user_id_new'].nunique()\nn_items = train['restaurant_id_new'].nunique()\ntxt_dim = train['text_emb'].iloc[0].shape[0]  # debería ser 768\n\n# ========================\n# MODELO MIX-TXTEN\n# ========================\nclass MixedModel(nn.Module):\n    def __init__(self, n_users, n_items, txt_dim, emb_dim=EMB_DIM):\n        super().__init__()\n        # embeddings clásicos user/item\n        self.uemb = nn.Embedding(n_users, emb_dim)\n        self.iemb = nn.Embedding(n_items, emb_dim)\n        nn.init.normal_(self.uemb.weight, mean=1.0, std=0.01)\n        nn.init.normal_(self.iemb.weight, mean=1.0, std=0.01)\n        # MLP final: concat [uemb | iemb | text_emb]\n        self.mlp = nn.Sequential(\n            nn.Linear(2*emb_dim + txt_dim, 100),\n            nn.ReLU(),\n            nn.Linear(100, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n    def forward(self, u, i, txt):\n        ue = self.uemb(u)\n        ie = self.iemb(i)\n        x = torch.cat([ue, ie, txt], dim=1)\n        return self.mlp(x).squeeze(1)\n\n# ========================\n# MÉTRICA RMSE\n# ========================\ndef compute_rmse(model, loader, criterion):\n    model.eval()\n    total, n = 0.0, 0\n    with torch.no_grad():\n        for u,i,txt,r in loader:\n            u,i,txt,r = u.to(DEVICE), i.to(DEVICE), txt.to(DEVICE), r.to(DEVICE)\n            pred = model(u,i,txt)\n            loss = criterion(pred, r)\n            total += loss.item() * r.size(0)\n            n += r.size(0)\n    return np.sqrt(total / n)\n\n# ========================\n# ENTRENAMIENTO\n# ========================\nresults = []\ncriterion = nn.MSELoss()\n\nfor lr in LR_LIST:\n    print(f\"\\nEntrenando MIX-TXTEN @ lr={lr}\")\n    t0 = time.time()\n\n    model = MixedModel(n_users, n_items, txt_dim).to(DEVICE)\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=WD)\n    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n\n    best_val, best_epoch, best_state = float('inf'), 0, None\n    train_hist, val_hist = [], []\n    early_cnt = 0\n\n    for epoch in tqdm(range(1, MAX_EPOCHS+1), desc=f\"MIX lr={lr}\"):\n        model.train()\n        running, count = 0.0, 0\n        for u,i,txt,r in train_loader:\n            u,i,txt,r = u.to(DEVICE), i.to(DEVICE), txt.to(DEVICE), r.to(DEVICE)\n            optimizer.zero_grad()\n            if use_amp:\n                with torch.cuda.amp.autocast():\n                    pred = model(u,i,txt)\n                    loss = criterion(pred, r)\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                pred = model(u,i,txt)\n                loss = criterion(pred, r)\n                loss.backward()\n                optimizer.step()\n            running += loss.item() * r.size(0)\n            count   += r.size(0)\n\n        train_rmse = np.sqrt(running/count)\n        val_rmse   = compute_rmse(model, val_loader, criterion)\n        train_hist.append(train_rmse)\n        val_hist.append(val_rmse)\n\n        if val_rmse < best_val:\n            best_val, best_epoch, best_state = val_rmse, epoch, model.state_dict()\n            early_cnt = 0\n        else:\n            early_cnt += 1\n            if early_cnt >= PATIENCE:\n                print(f\"Early stopping at epoch {epoch} (best {best_epoch})\")\n                break\n\n    # restaurar mejor\n    model.load_state_dict(best_state)\n    test_rmse = compute_rmse(model, test_loader, criterion)\n    dt = (time.time() - t0)/60\n\n    results.append({\n        \"Arquitectura\":  \"4-MIX-TXTEN-NY\",\n        \"lr\":            lr,\n        \"train_rmse\":    train_hist[best_epoch-1],\n        \"val_rmse\":      best_val,\n        \"test_rmse\":     test_rmse,\n        \"epochs\":        best_epoch,\n        \"train_time_m\":  round(dt,2)\n    })\n\n    # curva\n    plt.figure(figsize=(4,3))\n    plt.plot(train_hist, label=\"Train RMSE\")\n    plt.plot(val_hist,   label=\"Val RMSE\")\n    plt.axvline(best_epoch, ls=\"--\", c=\"gray\", label=\"Best ep.\")\n    plt.title(f\"MIX-TXTEN @ lr={lr}\")\n    plt.xlabel(\"Epoch\"); plt.ylabel(\"RMSE\")\n    plt.legend(loc=\"upper right\", fontsize=\"small\")\n    plt.tight_layout()\n    plt.savefig(f\"curve_mix_txten_lr{lr}.pdf\")\n    plt.close()\n\n# guardar resultados\ndf_res = pd.DataFrame(results)\ndf_res.to_excel(RESULTS_XLSX, index=False)\nprint(f\"\\nResultados en {RESULTS_XLSX}\")\nprint(f\"Tiempo total: {(time.time()-start_total)/60:.1f} min\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Similitud imagen-texto en español e inglés para Gijón mediante CLIP\n\nEn este análisis se ha comparado la similitud entre los embeddings de imagen y los de texto generados por CLIP, tanto en español como en inglés. A pesar de que CLIP fue entrenado principalmente en inglés, los resultados muestran que, en esta tarea, los textos en español presentan una mayor similitud con las imágenes. Esto sugiere que, cuando se trabaja con entradas multimodales (imagen + texto), puede ser preferible mantener el idioma original de las reseñas, ya que la traducción introduce variaciones que reducen la alineación semántica con el contenido visual.\n\nSin embargo, cuando se utiliza únicamente el texto como entrada, los experimentos muestran que los embeddings generados a partir de las reseñas traducidas al inglés permiten una mejor predicción del rating. Esto puede deberse a que CLIP y otros modelos preentrenados como BERT han sido optimizados en gran medida sobre corpora en inglés, por lo que su capacidad de comprensión y representación del lenguaje es más fina en ese idioma.\n\nEn resumen:\n\n- Para tareas multimodales (imagen + texto): los textos en español se alinean mejor con las imágenes.\n\n- Para tareas solo textuales: las reseñas traducidas al inglés producen embeddings más efectivos para la predicción.\n\nEsto refuerza la importancia de adaptar la representación del texto en función del tipo de entrada del modelo y del objetivo concreto de la tarea.","metadata":{}},{"cell_type":"code","source":"# Importación de librerías\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# 1) Filtrar filas con embeddings válidos\nmask_valid = (\n    train_df['image_emb'].apply(lambda x: isinstance(x, (list, np.ndarray))) &\n    train_df['text_emb_es'].apply(lambda x: isinstance(x, (list, np.ndarray))) &\n    train_df['text_emb_en'].apply(lambda x: isinstance(x, (list, np.ndarray)))\n)\ndf2 = train_df[mask_valid].reset_index(drop=True)\n\n# 2) Calcular similitudes\ndef compute_similarities(row):\n    img    = np.array(row['image_emb'], dtype=np.float32)\n    txt_es = np.array(row['text_emb_es'], dtype=np.float32)\n    txt_en = np.array(row['text_emb_en'], dtype=np.float32)\n    sim_es = cosine_similarity(img[None], txt_es[None])[0, 0]\n    sim_en = cosine_similarity(img[None], txt_en[None])[0, 0]\n    return pd.Series({'Español': sim_es, 'Inglés': sim_en})\n\n# 3) Aplicar función\nsims = df2.apply(compute_similarities, axis=1)\ndf_sims = pd.concat([df2[['reviewId']], sims], axis=1)\n\n# 4) Resumen estadístico\ndf_sims['en_better'] = df_sims['Inglés'] > df_sims['Español']\nsummary = {\n    'Mean sim_es': df_sims['Español'].mean(),\n    'Mean sim_en': df_sims['Inglés'].mean(),\n    'Proportion sim_en > sim_es': df_sims['en_better'].mean()\n}\nprint(pd.DataFrame([summary]))\n\n# 5) Boxplot\nplt.figure(figsize=(6, 4))\ndf_melted = df_sims.melt(value_vars=['Español', 'Inglés'], var_name='Idioma', value_name='Similitud')\nsns.boxplot(x='Idioma', y='Similitud', data=df_melted, palette=[\"#4B6C8F\", \"#9FB4C7\"])\n# Título: Distribución de similitud imagen-texto por idioma\nplt.tight_layout()\nplt.savefig(\"clip_boxplot_sim_idiomas.pdf\", format=\"pdf\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ejemplo de cálculo de la similitud imagen-texto con CLIP\n\nEn este bloque se analiza la alineación semántica entre imágenes y sus descripciones textuales mediante el modelo CLIP.\n\nLa matriz muestra las similitudes del coseno entre los embeddings generados por CLIP para cinco imágenes y cinco textos correspondientes. Se observa que los valores de la diagonal principal son los más altos en cada fila, lo que indica que cada imagen se alinea mejor con su descripción original. Esto confirma que CLIP es capaz de emparejar correctamente texto e imagen en estos ejemplos.\n\nAdemás, fuera de la diagonal, destacan algunas similitudes elevadas como la de \"Coliseum\" y \"Stadium\" (0.2712 y 0.2385), lo cual es coherente ya que ambos conceptos están semánticamente relacionados como estructuras arquitectónicas para eventos. Este patrón sugiere que el modelo también capta relaciones interconceptuales más allá del emparejamiento exacto, mostrando cierta capacidad de generalización.\n\nEn conjunto, la matriz valida el comportamiento esperado del modelo: puntuaciones más altas cuando la imagen y el texto corresponden, y puntuaciones moderadas o bajas en el resto, salvo en casos con cierta cercanía semántica.","metadata":{}},{"cell_type":"code","source":"# Importación de librerías\nimport torch\nimport clip\nfrom PIL import Image\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd\nimport numpy as np\n\n# Cargar modelo CLIP\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n# Frases y sus rutas de imágenes\ntexts = [\n    \"a photo of black seafood rice\",\n    \"a photo of a dog\",\n    \"a photo of some chips\",\n    \"a photo of the coliseum\",\n    \"a photo of a stadium\"\n]\n\nimage_paths = [\n    \"/kaggle/input/clipexamples/black_seafood_rice.jpg\",\n    \"/kaggle/input/clipexamples/dog.jpg\",\n    \"/kaggle/input/clipexamples/chips.jpg\",\n    \"/kaggle/input/clipexamples/coliseum.jpg\",\n    \"/kaggle/input/clipexamples/stadium.jpg\"\n]\n\n# Codificar textos\ntext_tokens = clip.tokenize(texts).to(device)\nwith torch.no_grad():\n    text_features = model.encode_text(text_tokens)\n\n# Codificar imágenes\nimage_features = []\nfor path in image_paths:\n    image = preprocess(Image.open(path)).unsqueeze(0).to(device)\n    with torch.no_grad():\n        feat = model.encode_image(image)\n        image_features.append(feat)\n\nimage_features = torch.cat(image_features, dim=0)\n\n# Normalizar vectores\nimage_features /= image_features.norm(dim=-1, keepdim=True)\ntext_features  /= text_features.norm(dim=-1, keepdim=True)\n\n# Convertir a NumPy\nimage_np = image_features.cpu().numpy()\ntext_np  = text_features.cpu().numpy()\n\n# Calcular matriz de similitud entre imágenes y textos\nsimilarity_matrix = cosine_similarity(image_np, text_np)\n\n# Crear DataFrame para visualización\nlabels = [\"Black seafood rice\", \"Dog\", \"Chips\", \"Coliseum\", \"Stadium\"]\ndf_sim = pd.DataFrame(similarity_matrix, index=[f\"{l} (img)\" for l in labels], columns=labels)\n\n# Imprimir tabla redondeada\nprint(\"\\nMatriz de similitud coseno entre imágenes y textos:\")\nprint(df_sim.round(4))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# División y recomposición de datos de Nueva York para procesamiento eficiente por lotes\n\nDado que el procesamiento de las reseñas con embeddings multimodales (especialmente la verificación de enlaces de imágenes y el cálculo de embeddings con CLIP) requería gran cantidad de memoria y tiempo, se observó que al trabajar con archivos grandes el sistema se volvía inestable o se colgaba. Para evitar estos problemas, se adoptó una estrategia de procesamiento por partes.\n\nPrimero, el dataset original se dividió en tres fragmentos de tamaño similar. Cada fragmento se procesó de forma independiente, lo que permitió verificar los enlaces de las imágenes y generar los embeddings sin sobrecargar la memoria del entorno de ejecución (por ejemplo, en Kaggle o Colab).\n\nUna vez completado el procesamiento parcial, los archivos resultantes se volvieron a recombinar para reconstruir el conjunto completo. Este proceso se aplicó tanto a los embeddings multimodales (texto + imagen) como a los embeddings textuales obtenidos en distintas fases.\n\nEste enfoque incremental resultó ser más robusto, escalable y menos propenso a errores, facilitando la generación y consolidación de los datos embebidos para su uso posterior en modelos de recomendación.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Cargar dataset original\ndf = pd.read_pickle(\"/kaggle/input/reviewspartny/reviews_part3.pkl\")\n\n# Dividir en 3 partes\nn = len(df)\nsplit1 = n // 3\nsplit2 = 2 * n // 3\n\ndf_part1 = df.iloc[:split1].reset_index(drop=True)\ndf_part2 = df.iloc[split1:split2].reset_index(drop=True)\ndf_part3 = df.iloc[split2:].reset_index(drop=True)\n\n# Guardar\ndf_part1.to_pickle(\"/kaggle/working/reviews_3_a.pkl\")\ndf_part2.to_pickle(\"/kaggle/working/reviews_3_b.pkl\")\ndf_part3.to_pickle(\"/kaggle/working/reviews_3_c.pkl\")\n\nprint(\"Dataset dividido en 3 partes y guardado en /kaggle/working/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Grupo 1\ndf1a = pd.read_pickle(\"/kaggle/working/reviews_out_ny1a.pkl\")\ndf1b = pd.read_pickle(\"/kaggle/working/reviews_out_ny1b.pkl\")\ndf1c = pd.read_pickle(\"/kaggle/working/reviews_out_ny1c.pkl\")\ndf1 = pd.concat([df1a, df1b, df1c], ignore_index=True)\ndf1.to_pickle(\"/kaggle/working/reviews_outemb_ny1.pkl\")\n\n# Grupo 2\ndf2a = pd.read_pickle(\"/kaggle/working/reviews_out_ny2a.pkl\")\ndf2b = pd.read_pickle(\"/kaggle/working/reviews_out_ny2b.pkl\")\ndf2c = pd.read_pickle(\"/kaggle/working/reviews_out_ny2c.pkl\")\ndf2 = pd.concat([df2a, df2b, df2c], ignore_index=True)\ndf2.to_pickle(\"/kaggle/working/reviews_outemb_ny2.pkl\")\n\n# Grupo 3\ndf3a = pd.read_pickle(\"/kaggle/working/reviews_out_ny3a.pkl\")\ndf3b = pd.read_pickle(\"/kaggle/working/reviews_out_ny3b.pkl\")\ndf3c = pd.read_pickle(\"/kaggle/working/reviews_out_ny3c.pkl\")\ndf3 = pd.concat([df3a, df3b, df3c], ignore_index=True)\ndf3.to_pickle(\"/kaggle/working/reviews_outemb_ny3.pkl\")\n\nprint(\"Conjuntos unidos y guardados como reviews_out_ny1/2/3.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Lista de archivos a unir\nfiles = [\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny1a_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny1b_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny1c_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny2a_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny2b_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny2c_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny3a_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny3b_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny3c_text.pkl\"\n]\n\n# Leer y concatenar todos\ndfs = [pd.read_pickle(f) for f in files]\ndf = pd.concat(dfs, ignore_index=True)\n\n# Guardar resultado combinado\ndf.to_pickle(\"/kaggle/working/reviews_ny_full_emb.pkl\")\nprint(\"✅ Pickles unidos en reviews_ny_full.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}