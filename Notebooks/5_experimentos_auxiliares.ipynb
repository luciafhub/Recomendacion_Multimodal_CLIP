{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Experimentos auxiliares para el TFG\n\nEste notebook recoge una serie de bloques de c√≥digo utilizados como apoyo durante el desarrollo del Trabajo de Fin de Grado. Incluye tareas complementarias como la traducci√≥n autom√°tica de rese√±as al ingl√©s, la generaci√≥n de embeddings textuales con BERT, el c√°lculo de similitudes entre im√°genes y textos con CLIP y el procesamiento por lotes de datos para evitar cuelgues del sistema. Aunque no forman parte del pipeline principal del sistema de recomendaci√≥n, estos experimentos y utilidades permitieron tomar decisiones clave sobre qu√© representaciones utilizar y c√≥mo preprocesar los datos de forma eficiente.","metadata":{}},{"cell_type":"markdown","source":"# Traducci√≥n Autom√°tica de Rese√±as con MarianMT\n\nEste bloque traduce autom√°ticamente los textos de rese√±as de restaurantes del espa√±ol al ingl√©s utilizando\nel modelo MarianMT de Helsinki-NLP. El objetivo principal es adaptar los datos a modelos de recomendaci√≥n\nmultimodal que utilizan embeddings generados con CLIP.\n\nCLIP fue entrenado mayoritariamente con datos en ingl√©s, por lo que se sospecha que las arquitecturas\nque usan embeddings CLIP de texto funcionar√°n mejor si las rese√±as est√°n traducidas al ingl√©s. Estas\narquitecturas incluyen:\n\n- CLIPTextRegressor: utiliza solo embeddings CLIP de texto.\n- CLIPMixedRegressor y FullModel: combinan embeddings CLIP de texto e imagen con identificadores de usuario e √≠tem.\n- MixedModel: mezcla codificaci√≥n tradicional (IDs) con embeddings de texto CLIP.\n\nTraducir los textos antes de generar los embeddings puede mejorar el rendimiento de estas arquitecturas,\nal alinear mejor el lenguaje de entrada con el vocabulario aprendido por CLIP durante su entrenamiento.","metadata":{}},{"cell_type":"code","source":"\n# ------------------------------\n# Importaci√≥n de librer√≠as\n# ------------------------------\n\nimport pandas as pd                 \nfrom tqdm.auto import tqdm           \nimport os                            \nimport torch                        \n\n# Modelos y pipeline de traducci√≥n\nfrom transformers import MarianMTModel, MarianTokenizer, pipeline \n\n# ------------------------------\n# Comprobaci√≥n del entorno\n# ------------------------------\n\n# Indica si hay GPU disponible para acelerar el proceso\nprint(\"GPU disponible:\", torch.cuda.is_available())\n\n# ------------------------------\n# 1) Carga del modelo y tokenizer\n# ------------------------------\n\n# Nombre del modelo espec√≠fico de espa√±ol a ingl√©s de Helsinki-NLP\nmodel_name = \"Helsinki-NLP/opus-mt-es-en\"\n\n# Se carga el tokenizer con opci√≥n r√°pida (m√°s eficiente)\ntokenizer = MarianTokenizer.from_pretrained(model_name, use_fast=True)\n\n# Se carga el modelo y se transfiere a GPU (cuda)\nmodel = MarianMTModel.from_pretrained(model_name).to(\"cuda\")\n\n# ------------------------------\n# 2) Creaci√≥n del pipeline de traducci√≥n\n# ------------------------------\n\n# Se construye el pipeline indicando expl√≠citamente:\n# - Modelo y tokenizer\n# - Uso de GPU con device=0 (primera GPU disponible)\n# - batch_size grande para acelerar, si hay suficiente VRAM\n# - max_length y num_beams ajustados para rapidez\ntranslator = pipeline(\n    \"translation\",\n    model=model,\n    tokenizer=tokenizer,\n    device=0,               # GPU 0\n    framework=\"pt\",         # PyTorch\n    batch_size=64,\n    max_length=512,\n    num_beams=1,            # b√∫squeda sin beams para mayor velocidad\n    do_sample=False         # desactiva muestreo aleatorio\n)\n\n# ------------------------------\n# 3) Funci√≥n de traducci√≥n por batches\n# ------------------------------\n\ndef translate_dataset_batched(name, batch_size=64, max_chars=512):\n    \"\"\"\n    Traduce un dataset .pkl con una columna 'text' del espa√±ol al ingl√©s.\n    Guarda el resultado en otro archivo con sufijo '_en.pkl'.\n\n    Par√°metros:\n    - name: nombre base del archivo (sin .pkl)\n    - batch_size: n√∫mero de ejemplos a traducir por lote\n    - max_chars: longitud m√°xima de texto por entrada (truncado)\n    \"\"\"\n\n    inp = f\"{name}.pkl\"\n    out = f\"{name}_en.pkl\"\n\n    print(f\"\\n>> Cargando {inp}‚Ä¶\")\n    df = pd.read_pickle(inp)\n\n    # Se obtienen los textos, se rellenan nulos y se convierten a string\n    texts = df[\"text\"].fillna(\"\").astype(str).tolist()\n\n    # Truncado previo para asegurar compatibilidad con el modelo\n    texts = [\n        (t if len(t) <= max_chars else t[:max_chars] + \"...\")\n        for t in texts\n    ]\n\n    # Traducci√≥n por lotes\n    translated = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Traduciendo {name}\"):\n        batch = texts[i : i + batch_size]\n        outputs = translator(batch)\n        translated.extend([o[\"translation_text\"] for o in outputs])\n\n    # Se a√±ade la columna traducida al DataFrame\n    df[\"text_en\"] = translated\n\n    print(f\">> Guardando en {out}‚Ä¶\")\n    df.to_pickle(out)\n    print(f\"‚úÖ {name} traducido y guardado.\")\n\n# ------------------------------\n# 4) Traducci√≥n de datasets disponibles\n# ------------------------------\n\n# Se aplica la funci√≥n a los conjuntos de entrenamiento, validaci√≥n y prueba\nfor split in [\"train_strict\", \"val_strict\", \"test_strict\"]:\n    if os.path.isfile(f\"{split}.pkl\"):\n        translate_dataset_batched(split)\n    else:\n        print(f\"[SKIP] No existe {split}.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generaci√≥n de Embeddings Textuales con BERT\n\nEste bloque genera representaciones vectoriales (embeddings) a partir de rese√±as escritas en texto,\nutilizando el modelo preentrenado BERT (bert-base-uncased). A diferencia de CLIP, que genera\nembeddings multimodales (texto + imagen), BERT est√° optimizado exclusivamente para lenguaje natural,\npor lo que puede capturar mejor las relaciones sem√°nticas presentes en las rese√±as.\n\n\nDurante los experimentos se observ√≥ que las im√°genes asociadas a los restaurantes no aportaban informaci√≥n relevante para la tarea de recomendaci√≥n. En cambio, las rese√±as textuales demostraron tener un alto poder predictivo. Por ello, se decidi√≥ probar un modelo alternativo especializado en lenguaje natural: BERT, con la hip√≥tesis de que generar√≠a embeddings m√°s representativos que CLIP.\n\nLos resultados confirmaron esta sospecha: los embeddings generados con BERT ofrecieron un mejor rendimiento que los obtenidos con CLIP. Esto sugiere que, en este dominio concreto, el contenido ling√º√≠stico de las rese√±as es el principal factor que determina la satisfacci√≥n del usuario.\n\nEl objetivo de este script es, por tanto, reemplazar los embeddings generados con CLIP por embeddings obtenidos con BERT, que se almacenan en una nueva columna llamada \"text_emb\" dentro del DataFrame.","metadata":{}},{"cell_type":"code","source":"# ------------------------------\n# 0) Instalaci√≥n de librer√≠as\n# ------------------------------\n# Se instalan Transformers y TensorFlow (aunque este script no necesita TF)\n# En entornos como Kaggle o Colab, TensorFlow puede generar conflictos si no se usa\n!pip install -q tensorflow tensorflow-estimator\n!pip install -q transformers\n\n# ------------------------------\n# 1) Importaci√≥n de librer√≠as\n# ------------------------------\n\nimport pandas as pd                            # Manipulaci√≥n de DataFrames\nimport numpy as np                             # Operaciones num√©ricas\nimport torch                                   # Uso de GPU y modelo BERT\nfrom transformers import AutoTokenizer, AutoModel  # Carga del modelo BERT\nfrom tqdm.auto import tqdm                     # Barra de progreso\n\n# ------------------------------\n# 2) Configuraci√≥n del entorno\n# ------------------------------\n\ndevice     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU si est√° disponible\nMODEL_NAME = \"bert-base-uncased\"   # Modelo preentrenado de BERT (casing ignorado)\nBATCH_SIZE = 32                    # Procesamiento por lotes\nMAX_LEN    = 128                   # M√°ximo n√∫mero de tokens por texto\n\n# ------------------------------\n# 3) Carga de modelo y tokenizador\n# ------------------------------\n\n# Tokenizador que divide el texto en subpalabras y a√±ade tokens especiales\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Modelo BERT que genera los embeddings\nmodel = AutoModel.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()  # Se desactiva el modo entrenamiento (no dropout, no actualiza pesos)\n\n# ------------------------------\n# 4) Mean Pooling\n# ------------------------------\n\ndef mean_pooling(hidden_states, attention_mask):\n    \"\"\"\n    Calcula el embedding medio de las palabras reales (excluyendo padding).\n    \n    hidden_states: salida de BERT (batch_size, seq_len, hidden_dim)\n    attention_mask: m√°scara con 1s en posiciones v√°lidas y 0s en padding\n    \"\"\"\n    mask = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n    return (hidden_states * mask).sum(1) / mask.sum(1)  # Promedio solo donde mask=1\n\n# ------------------------------\n# 5) Funci√≥n para obtener embeddings BERT\n# ------------------------------\n\ndef get_bert_embeddings(texts, desc=\"\"):\n    \"\"\"\n    Procesa una lista de textos, tokeniza y genera embeddings con BERT.\n    \n    Par√°metros:\n    - texts: lista de cadenas de texto\n    - desc: descripci√≥n para mostrar en la barra de progreso\n\n    Devuelve:\n    - matriz NumPy (N, hidden_dim) con los embeddings\n    \"\"\"\n    all_embs = []\n\n    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=f\"üìù Embedding {desc}\"):\n        batch = texts[i : i + BATCH_SIZE]\n\n        # Tokenizaci√≥n por lotes con padding y truncado\n        enc = tokenizer(\n            batch,\n            padding=True,\n            truncation=True,\n            max_length=MAX_LEN,\n            return_tensors=\"pt\"\n        )\n\n        input_ids      = enc[\"input_ids\"].to(device)\n        attention_mask = enc[\"attention_mask\"].to(device)\n\n        # C√°lculo de embeddings sin actualizar el modelo\n        with torch.no_grad():\n            out = model(input_ids=input_ids, attention_mask=attention_mask)\n\n        # Aplicar mean pooling\n        emb = mean_pooling(out.last_hidden_state, attention_mask)\n\n        # Convertir a NumPy y guardar\n        all_embs.append(emb.cpu().numpy())\n\n    return np.vstack(all_embs)  # (N, dim)\n\n# ------------------------------\n# 6) Procesamiento de archivos\n# ------------------------------\n\n# Diccionario con rutas a los archivos .pkl de cada split\nFILES = {\n    \"train\": \"/kaggle/input/ny8010/train_v80.pkl\",\n    \"val\":   \"/kaggle/input/ny8010/val_v10.pkl\",\n    \"test\":  \"/kaggle/input/ny8010/test_v10.pkl\"\n}\n\n# Proceso por cada split\nfor split, path in FILES.items():\n    print(f\"\\nüîÑ Procesando {split} ({path})‚Ä¶\")\n\n    # Carga del DataFrame\n    df = pd.read_pickle(path)\n\n    # Normalizaci√≥n de textos: asegurar strings y eliminar NaNs\n    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n\n    # Obtener embeddings\n    embs = get_bert_embeddings(df[\"text\"].tolist(), split)\n\n    # A√±adir al DataFrame\n    df[\"text_emb_bert\"] = list(embs)\n\n    # Guardar archivo enriquecido\n    out = f\"/kaggle/working/{split}_bert_emb.pkl\"\n    df.to_pickle(out)\n    print(f\"‚úÖ Guardado embeddings en {out}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Arquitectura con la que se verefica que BERT da mejor resultado que CLIP\n\nSe prob√≥ la arquitectura 4-MIX-TXT, que combina los identificadores de usuario e √≠tem con los embeddings textuales de las rese√±as. Inicialmente, esta arquitectura se entren√≥ utilizando embeddings generados con CLIP. Sin embargo, los resultados no fueron del todo satisfactorios: el mejor RMSE en validaci√≥n fue de 6.68 para el conjunto de Nueva York.\n\nCon el objetivo de mejorar la calidad de los embeddings textuales, se sustituy√≥ CLIP por BERT (bert-base-uncased), un modelo especializado en lenguaje natural. Esta modificaci√≥n tuvo un impacto positivo claro: el mejor RMSE en validaci√≥n descendi√≥ a 6.03 en Nueva York, una mejora significativa en la precisi√≥n del sistema de recomendaci√≥n.\n\nEste experimento demuestra que, en esta tarea concreta, las im√°genes no aportan valor predictivo, y que el texto ‚Äîrepresentado mediante embeddings BERT‚Äî resulta ser el componente clave. Por tanto, reemplazar CLIP por BERT mejora la capacidad del modelo para anticipar la satisfacci√≥n del usuario.","metadata":{}},{"cell_type":"code","source":"import time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# ========================\n# CONFIGURACI√ìN GENERAL\n# ========================\nstart_total = time.time()\n\nDEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nuse_amp    = DEVICE == \"cuda\"\nEMB_DIM    = 50                 # dimensi√≥n de user/item embeddings\nBATCH_SIZE = 128\nLR_LIST    = [1e-5, 1e-4, 5e-4, 1e-3]\nMAX_EPOCHS = 1000\nPATIENCE   = 10\nWD         = 1e-6               # weight_decay\nRESULTS_XLSX = \"Arquitectura_4_MIX_TXTEN_NY_BERT.xlsx\"\n\n# ========================\n# DATASETS\n# ========================\nclass MixedDataset(Dataset):\n    def __init__(self, df):\n        # usuario, restaurante\n        self.u = torch.tensor(df['user_id_new'].values, dtype=torch.long)\n        self.i = torch.tensor(df['restaurant_id_new'].values, dtype=torch.long)\n        # embeddings BERT de texto (dim 768)\n        self.x = torch.tensor(np.vstack(df['text_emb'].values), dtype=torch.float32)\n        # rating\n        self.r = torch.tensor(df['rating'].values, dtype=torch.float32)\n    def __len__(self):\n        return len(self.r)\n    def __getitem__(self, idx):\n        return self.u[idx], self.i[idx], self.x[idx], self.r[idx]\n\n# cargar DataFrames\ntrain = pd.read_pickle(\"/kaggle/input/ny-bert/train_bert_emb.pkl\")\nval   = pd.read_pickle(\"/kaggle/input/ny-bert/val_bert_emb.pkl\")\ntest  = pd.read_pickle(\"/kaggle/input/ny-bert/test_bert_emb.pkl\")\n\n# crear DataLoaders\ntrain_loader = DataLoader(MixedDataset(train), batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(MixedDataset(val),   batch_size=BATCH_SIZE, shuffle=False)\ntest_loader  = DataLoader(MixedDataset(test),  batch_size=BATCH_SIZE, shuffle=False)\n\nn_users = train['user_id_new'].nunique()\nn_items = train['restaurant_id_new'].nunique()\ntxt_dim = train['text_emb'].iloc[0].shape[0]  # deber√≠a ser 768\n\n# ========================\n# MODELO MIX-TXTEN\n# ========================\nclass MixedModel(nn.Module):\n    def __init__(self, n_users, n_items, txt_dim, emb_dim=EMB_DIM):\n        super().__init__()\n        # embeddings cl√°sicos user/item\n        self.uemb = nn.Embedding(n_users, emb_dim)\n        self.iemb = nn.Embedding(n_items, emb_dim)\n        nn.init.normal_(self.uemb.weight, mean=1.0, std=0.01)\n        nn.init.normal_(self.iemb.weight, mean=1.0, std=0.01)\n        # MLP final: concat [uemb | iemb | text_emb]\n        self.mlp = nn.Sequential(\n            nn.Linear(2*emb_dim + txt_dim, 100),\n            nn.ReLU(),\n            nn.Linear(100, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n    def forward(self, u, i, txt):\n        ue = self.uemb(u)\n        ie = self.iemb(i)\n        x = torch.cat([ue, ie, txt], dim=1)\n        return self.mlp(x).squeeze(1)\n\n# ========================\n# M√âTRICA RMSE\n# ========================\ndef compute_rmse(model, loader, criterion):\n    model.eval()\n    total, n = 0.0, 0\n    with torch.no_grad():\n        for u,i,txt,r in loader:\n            u,i,txt,r = u.to(DEVICE), i.to(DEVICE), txt.to(DEVICE), r.to(DEVICE)\n            pred = model(u,i,txt)\n            loss = criterion(pred, r)\n            total += loss.item() * r.size(0)\n            n += r.size(0)\n    return np.sqrt(total / n)\n\n# ========================\n# ENTRENAMIENTO\n# ========================\nresults = []\ncriterion = nn.MSELoss()\n\nfor lr in LR_LIST:\n    print(f\"\\nEntrenando MIX-TXTEN @ lr={lr}\")\n    t0 = time.time()\n\n    model = MixedModel(n_users, n_items, txt_dim).to(DEVICE)\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=WD)\n    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n\n    best_val, best_epoch, best_state = float('inf'), 0, None\n    train_hist, val_hist = [], []\n    early_cnt = 0\n\n    for epoch in tqdm(range(1, MAX_EPOCHS+1), desc=f\"MIX lr={lr}\"):\n        model.train()\n        running, count = 0.0, 0\n        for u,i,txt,r in train_loader:\n            u,i,txt,r = u.to(DEVICE), i.to(DEVICE), txt.to(DEVICE), r.to(DEVICE)\n            optimizer.zero_grad()\n            if use_amp:\n                with torch.cuda.amp.autocast():\n                    pred = model(u,i,txt)\n                    loss = criterion(pred, r)\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                pred = model(u,i,txt)\n                loss = criterion(pred, r)\n                loss.backward()\n                optimizer.step()\n            running += loss.item() * r.size(0)\n            count   += r.size(0)\n\n        train_rmse = np.sqrt(running/count)\n        val_rmse   = compute_rmse(model, val_loader, criterion)\n        train_hist.append(train_rmse)\n        val_hist.append(val_rmse)\n\n        if val_rmse < best_val:\n            best_val, best_epoch, best_state = val_rmse, epoch, model.state_dict()\n            early_cnt = 0\n        else:\n            early_cnt += 1\n            if early_cnt >= PATIENCE:\n                print(f\"Early stopping at epoch {epoch} (best {best_epoch})\")\n                break\n\n    # restaurar mejor\n    model.load_state_dict(best_state)\n    test_rmse = compute_rmse(model, test_loader, criterion)\n    dt = (time.time() - t0)/60\n\n    results.append({\n        \"Arquitectura\":  \"4-MIX-TXTEN-NY\",\n        \"lr\":            lr,\n        \"train_rmse\":    train_hist[best_epoch-1],\n        \"val_rmse\":      best_val,\n        \"test_rmse\":     test_rmse,\n        \"epochs\":        best_epoch,\n        \"train_time_m\":  round(dt,2)\n    })\n\n    # curva\n    plt.figure(figsize=(4,3))\n    plt.plot(train_hist, label=\"Train RMSE\")\n    plt.plot(val_hist,   label=\"Val RMSE\")\n    plt.axvline(best_epoch, ls=\"--\", c=\"gray\", label=\"Best ep.\")\n    plt.title(f\"MIX-TXTEN @ lr={lr}\")\n    plt.xlabel(\"Epoch\"); plt.ylabel(\"RMSE\")\n    plt.legend(loc=\"upper right\", fontsize=\"small\")\n    plt.tight_layout()\n    plt.savefig(f\"curve_mix_txten_lr{lr}.pdf\")\n    plt.close()\n\n# guardar resultados\ndf_res = pd.DataFrame(results)\ndf_res.to_excel(RESULTS_XLSX, index=False)\nprint(f\"\\nResultados en {RESULTS_XLSX}\")\nprint(f\"Tiempo total: {(time.time()-start_total)/60:.1f} min\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Similitud imagen-texto en espa√±ol e ingl√©s para Gij√≥n mediante CLIP\n\nEn este an√°lisis se ha comparado la similitud entre los embeddings de imagen y los de texto generados por CLIP, tanto en espa√±ol como en ingl√©s. A pesar de que CLIP fue entrenado principalmente en ingl√©s, los resultados muestran que, en esta tarea, los textos en espa√±ol presentan una mayor similitud con las im√°genes. Esto sugiere que, cuando se trabaja con entradas multimodales (imagen + texto), puede ser preferible mantener el idioma original de las rese√±as, ya que la traducci√≥n introduce variaciones que reducen la alineaci√≥n sem√°ntica con el contenido visual.\n\nSin embargo, cuando se utiliza √∫nicamente el texto como entrada, los experimentos muestran que los embeddings generados a partir de las rese√±as traducidas al ingl√©s permiten una mejor predicci√≥n del rating. Esto puede deberse a que CLIP y otros modelos preentrenados como BERT han sido optimizados en gran medida sobre corpora en ingl√©s, por lo que su capacidad de comprensi√≥n y representaci√≥n del lenguaje es m√°s fina en ese idioma.\n\nEn resumen:\n\n- Para tareas multimodales (imagen + texto): los textos en espa√±ol se alinean mejor con las im√°genes.\n\n- Para tareas solo textuales: las rese√±as traducidas al ingl√©s producen embeddings m√°s efectivos para la predicci√≥n.\n\nEsto refuerza la importancia de adaptar la representaci√≥n del texto en funci√≥n del tipo de entrada del modelo y del objetivo concreto de la tarea.","metadata":{}},{"cell_type":"code","source":"# Importaci√≥n de librer√≠as\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# 1) Filtrar filas con embeddings v√°lidos\nmask_valid = (\n    train_df['image_emb'].apply(lambda x: isinstance(x, (list, np.ndarray))) &\n    train_df['text_emb_es'].apply(lambda x: isinstance(x, (list, np.ndarray))) &\n    train_df['text_emb_en'].apply(lambda x: isinstance(x, (list, np.ndarray)))\n)\ndf2 = train_df[mask_valid].reset_index(drop=True)\n\n# 2) Calcular similitudes\ndef compute_similarities(row):\n    img    = np.array(row['image_emb'], dtype=np.float32)\n    txt_es = np.array(row['text_emb_es'], dtype=np.float32)\n    txt_en = np.array(row['text_emb_en'], dtype=np.float32)\n    sim_es = cosine_similarity(img[None], txt_es[None])[0, 0]\n    sim_en = cosine_similarity(img[None], txt_en[None])[0, 0]\n    return pd.Series({'Espa√±ol': sim_es, 'Ingl√©s': sim_en})\n\n# 3) Aplicar funci√≥n\nsims = df2.apply(compute_similarities, axis=1)\ndf_sims = pd.concat([df2[['reviewId']], sims], axis=1)\n\n# 4) Resumen estad√≠stico\ndf_sims['en_better'] = df_sims['Ingl√©s'] > df_sims['Espa√±ol']\nsummary = {\n    'Mean sim_es': df_sims['Espa√±ol'].mean(),\n    'Mean sim_en': df_sims['Ingl√©s'].mean(),\n    'Proportion sim_en > sim_es': df_sims['en_better'].mean()\n}\nprint(pd.DataFrame([summary]))\n\n# 5) Boxplot\nplt.figure(figsize=(6, 4))\ndf_melted = df_sims.melt(value_vars=['Espa√±ol', 'Ingl√©s'], var_name='Idioma', value_name='Similitud')\nsns.boxplot(x='Idioma', y='Similitud', data=df_melted, palette=[\"#4B6C8F\", \"#9FB4C7\"])\n# T√≠tulo: Distribuci√≥n de similitud imagen-texto por idioma\nplt.tight_layout()\nplt.savefig(\"clip_boxplot_sim_idiomas.pdf\", format=\"pdf\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ejemplo de c√°lculo de la similitud imagen-texto con CLIP\n\nEn este bloque se analiza la alineaci√≥n sem√°ntica entre im√°genes y sus descripciones textuales mediante el modelo CLIP.\n\nLa matriz muestra las similitudes del coseno entre los embeddings generados por CLIP para cinco im√°genes y cinco textos correspondientes. Se observa que los valores de la diagonal principal son los m√°s altos en cada fila, lo que indica que cada imagen se alinea mejor con su descripci√≥n original. Esto confirma que CLIP es capaz de emparejar correctamente texto e imagen en estos ejemplos.\n\nAdem√°s, fuera de la diagonal, destacan algunas similitudes elevadas como la de \"Coliseum\" y \"Stadium\" (0.2712 y 0.2385), lo cual es coherente ya que ambos conceptos est√°n sem√°nticamente relacionados como estructuras arquitect√≥nicas para eventos. Este patr√≥n sugiere que el modelo tambi√©n capta relaciones interconceptuales m√°s all√° del emparejamiento exacto, mostrando cierta capacidad de generalizaci√≥n.\n\nEn conjunto, la matriz valida el comportamiento esperado del modelo: puntuaciones m√°s altas cuando la imagen y el texto corresponden, y puntuaciones moderadas o bajas en el resto, salvo en casos con cierta cercan√≠a sem√°ntica.","metadata":{}},{"cell_type":"code","source":"# Importaci√≥n de librer√≠as\nimport torch\nimport clip\nfrom PIL import Image\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd\nimport numpy as np\n\n# Cargar modelo CLIP\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n# Frases y sus rutas de im√°genes\ntexts = [\n    \"a photo of black seafood rice\",\n    \"a photo of a dog\",\n    \"a photo of some chips\",\n    \"a photo of the coliseum\",\n    \"a photo of a stadium\"\n]\n\nimage_paths = [\n    \"/kaggle/input/clipexamples/black_seafood_rice.jpg\",\n    \"/kaggle/input/clipexamples/dog.jpg\",\n    \"/kaggle/input/clipexamples/chips.jpg\",\n    \"/kaggle/input/clipexamples/coliseum.jpg\",\n    \"/kaggle/input/clipexamples/stadium.jpg\"\n]\n\n# Codificar textos\ntext_tokens = clip.tokenize(texts).to(device)\nwith torch.no_grad():\n    text_features = model.encode_text(text_tokens)\n\n# Codificar im√°genes\nimage_features = []\nfor path in image_paths:\n    image = preprocess(Image.open(path)).unsqueeze(0).to(device)\n    with torch.no_grad():\n        feat = model.encode_image(image)\n        image_features.append(feat)\n\nimage_features = torch.cat(image_features, dim=0)\n\n# Normalizar vectores\nimage_features /= image_features.norm(dim=-1, keepdim=True)\ntext_features  /= text_features.norm(dim=-1, keepdim=True)\n\n# Convertir a NumPy\nimage_np = image_features.cpu().numpy()\ntext_np  = text_features.cpu().numpy()\n\n# Calcular matriz de similitud entre im√°genes y textos\nsimilarity_matrix = cosine_similarity(image_np, text_np)\n\n# Crear DataFrame para visualizaci√≥n\nlabels = [\"Black seafood rice\", \"Dog\", \"Chips\", \"Coliseum\", \"Stadium\"]\ndf_sim = pd.DataFrame(similarity_matrix, index=[f\"{l} (img)\" for l in labels], columns=labels)\n\n# Imprimir tabla redondeada\nprint(\"\\nMatriz de similitud coseno entre im√°genes y textos:\")\nprint(df_sim.round(4))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Divisi√≥n y recomposici√≥n de datos de Nueva York para procesamiento eficiente por lotes\n\nDado que el procesamiento de las rese√±as con embeddings multimodales (especialmente la verificaci√≥n de enlaces de im√°genes y el c√°lculo de embeddings con CLIP) requer√≠a gran cantidad de memoria y tiempo, se observ√≥ que al trabajar con archivos grandes el sistema se volv√≠a inestable o se colgaba. Para evitar estos problemas, se adopt√≥ una estrategia de procesamiento por partes.\n\nPrimero, el dataset original se dividi√≥ en tres fragmentos de tama√±o similar. Cada fragmento se proces√≥ de forma independiente, lo que permiti√≥ verificar los enlaces de las im√°genes y generar los embeddings sin sobrecargar la memoria del entorno de ejecuci√≥n (por ejemplo, en Kaggle o Colab).\n\nUna vez completado el procesamiento parcial, los archivos resultantes se volvieron a recombinar para reconstruir el conjunto completo. Este proceso se aplic√≥ tanto a los embeddings multimodales (texto + imagen) como a los embeddings textuales obtenidos en distintas fases.\n\nEste enfoque incremental result√≥ ser m√°s robusto, escalable y menos propenso a errores, facilitando la generaci√≥n y consolidaci√≥n de los datos embebidos para su uso posterior en modelos de recomendaci√≥n.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Cargar dataset original\ndf = pd.read_pickle(\"/kaggle/input/reviewspartny/reviews_part3.pkl\")\n\n# Dividir en 3 partes\nn = len(df)\nsplit1 = n // 3\nsplit2 = 2 * n // 3\n\ndf_part1 = df.iloc[:split1].reset_index(drop=True)\ndf_part2 = df.iloc[split1:split2].reset_index(drop=True)\ndf_part3 = df.iloc[split2:].reset_index(drop=True)\n\n# Guardar\ndf_part1.to_pickle(\"/kaggle/working/reviews_3_a.pkl\")\ndf_part2.to_pickle(\"/kaggle/working/reviews_3_b.pkl\")\ndf_part3.to_pickle(\"/kaggle/working/reviews_3_c.pkl\")\n\nprint(\"Dataset dividido en 3 partes y guardado en /kaggle/working/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Grupo 1\ndf1a = pd.read_pickle(\"/kaggle/working/reviews_out_ny1a.pkl\")\ndf1b = pd.read_pickle(\"/kaggle/working/reviews_out_ny1b.pkl\")\ndf1c = pd.read_pickle(\"/kaggle/working/reviews_out_ny1c.pkl\")\ndf1 = pd.concat([df1a, df1b, df1c], ignore_index=True)\ndf1.to_pickle(\"/kaggle/working/reviews_outemb_ny1.pkl\")\n\n# Grupo 2\ndf2a = pd.read_pickle(\"/kaggle/working/reviews_out_ny2a.pkl\")\ndf2b = pd.read_pickle(\"/kaggle/working/reviews_out_ny2b.pkl\")\ndf2c = pd.read_pickle(\"/kaggle/working/reviews_out_ny2c.pkl\")\ndf2 = pd.concat([df2a, df2b, df2c], ignore_index=True)\ndf2.to_pickle(\"/kaggle/working/reviews_outemb_ny2.pkl\")\n\n# Grupo 3\ndf3a = pd.read_pickle(\"/kaggle/working/reviews_out_ny3a.pkl\")\ndf3b = pd.read_pickle(\"/kaggle/working/reviews_out_ny3b.pkl\")\ndf3c = pd.read_pickle(\"/kaggle/working/reviews_out_ny3c.pkl\")\ndf3 = pd.concat([df3a, df3b, df3c], ignore_index=True)\ndf3.to_pickle(\"/kaggle/working/reviews_outemb_ny3.pkl\")\n\nprint(\"Conjuntos unidos y guardados como reviews_out_ny1/2/3.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Lista de archivos a unir\nfiles = [\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny1a_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny1b_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny1c_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny2a_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny2b_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny2c_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny3a_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny3b_text.pkl\",\n    \"/kaggle/input/ny-emb-fin/reviews_out_ny3c_text.pkl\"\n]\n\n# Leer y concatenar todos\ndfs = [pd.read_pickle(f) for f in files]\ndf = pd.concat(dfs, ignore_index=True)\n\n# Guardar resultado combinado\ndf.to_pickle(\"/kaggle/working/reviews_ny_full_emb.pkl\")\nprint(\"‚úÖ Pickles unidos en reviews_ny_full.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}